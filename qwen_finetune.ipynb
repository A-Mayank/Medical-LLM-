{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042853c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Origin Medical\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cb9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "TRAIN_DATASET_PATH = \"new_dataset\\medicare_110k_train.json\"  # Update this path\n",
    "TEST_DATASET_PATH = \"new_dataset/medicare_110k_test.json\"  # Update this path\n",
    "OUTPUT_DIR = \"./qwen2.5-medical-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc29772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking CUDA availability...\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# CUDA verification\n",
    "print(\"üîç Checking CUDA availability...\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available! Training will be very slow on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb68701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8a5abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "‚úÖ Model is running on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Verify model is on GPU\n",
    "print(f\"Model device: {model.device}\")\n",
    "if next(model.parameters()).is_cuda:\n",
    "    print(\"‚úÖ Model is running on GPU!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Model is on CPU - training will be slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1046e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "Trying JSONL format for medicare_110k_train.json...\n",
      "Processed 10000 samples...\n",
      "Processed 20000 samples...\n",
      "Processed 30000 samples...\n",
      "Processed 40000 samples...\n",
      "Processed 50000 samples...\n",
      "Processed 60000 samples...\n",
      "Processed 70000 samples...\n",
      "Processed 80000 samples...\n",
      "Processed 90000 samples...\n",
      "Processed 100000 samples...\n",
      "Successfully formatted 106556 conversations from medicare_110k_train.json\n",
      "Loading test dataset...\n",
      "Trying JSONL format for medicare_110k_test.json...\n",
      "Successfully formatted 5000 conversations from medicare_110k_test.json\n"
     ]
    }
   ],
   "source": [
    "# Load dataset function\n",
    "def load_conversations(file_path, max_samples=None):\n",
    "    \"\"\"Load conversations from JSON file\"\"\"\n",
    "    formatted_conversations = []\n",
    "\n",
    "    try:\n",
    "        # Try reading as JSON array first\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        print(f\"Loaded {len(data)} samples from {os.path.basename(file_path)}\")\n",
    "\n",
    "        samples_to_process = data[:max_samples] if max_samples else data\n",
    "\n",
    "        for i, conversation in enumerate(samples_to_process):\n",
    "            if \"Conversation\" in conversation:\n",
    "                conv_text = conversation[\"Conversation\"]\n",
    "\n",
    "                # Extract human and AI parts\n",
    "                if \"[|Human|]\" in conv_text and \"[|AI|]\" in conv_text:\n",
    "                    parts = conv_text.split(\"[|Human|]\")[1]\n",
    "                    human_part, ai_part = parts.split(\"[|AI|]\")\n",
    "\n",
    "                    human_part = human_part.strip()\n",
    "                    ai_part = ai_part.strip()\n",
    "\n",
    "                    # Format for Qwen2.5 chat\n",
    "                    formatted_text = f\"<|im_start|>user\\n{human_part}<|im_end|>\\n<|im_start|>assistant\\n{ai_part}<|im_end|>\"\n",
    "                    formatted_conversations.append({\"text\": formatted_text})\n",
    "\n",
    "            if i % 10000 == 0 and i > 0:\n",
    "                print(f\"Processed {i} samples...\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON array fails, try reading as JSONL (one JSON per line)\n",
    "        print(f\"Trying JSONL format for {os.path.basename(file_path)}...\")\n",
    "        formatted_conversations = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if max_samples and i >= max_samples:\n",
    "                    break\n",
    "\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        conversation = json.loads(line)\n",
    "                        if \"Conversation\" in conversation:\n",
    "                            conv_text = conversation[\"Conversation\"]\n",
    "\n",
    "                            if \"[|Human|]\" in conv_text and \"[|AI|]\" in conv_text:\n",
    "                                parts = conv_text.split(\"[|Human|]\")[1]\n",
    "                                human_part, ai_part = parts.split(\"[|AI|]\")\n",
    "\n",
    "                                human_part = human_part.strip()\n",
    "                                ai_part = ai_part.strip()\n",
    "\n",
    "                                formatted_text = f\"<|im_start|>user\\n{human_part}<|im_end|>\\n<|im_start|>assistant\\n{ai_part}<|im_end|>\"\n",
    "                                formatted_conversations.append({\"text\": formatted_text})\n",
    "\n",
    "                        if i % 10000 == 0 and i > 0:\n",
    "                            print(f\"Processed {i} samples...\")\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Skipping line {i+1}: {e}\")\n",
    "                        continue\n",
    "\n",
    "    print(\n",
    "        f\"Successfully formatted {len(formatted_conversations)} conversations from {os.path.basename(file_path)}\"\n",
    "    )\n",
    "    return formatted_conversations\n",
    "\n",
    "\n",
    "# Load train and test datasets\n",
    "print(\"Loading training dataset...\")\n",
    "train_conversations = load_conversations(\n",
    "    TRAIN_DATASET_PATH, max_samples=None\n",
    ")  # Use all training samples\n",
    "\n",
    "print(\"Loading test dataset...\")\n",
    "test_conversations = load_conversations(\n",
    "    TEST_DATASET_PATH, max_samples=5000\n",
    ")  # Limit test samples if needed\n",
    "\n",
    "if not train_conversations:\n",
    "    print(\"No valid training conversations found!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7a7da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Training samples: 106556\n",
      "üß™ Test samples: 5000\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = Dataset.from_list(train_conversations)\n",
    "eval_dataset = Dataset.from_list(test_conversations) if test_conversations else None\n",
    "\n",
    "print(f\"üìö Training samples: {len(train_dataset)}\")\n",
    "print(f\"üß™ Test samples: {len(eval_dataset) if eval_dataset else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5f1ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 106556/106556 [00:44<00:00, 2421.39 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:01<00:00, 2575.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "if eval_dataset:\n",
    "    tokenized_eval = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names,\n",
    "        batch_size=1000,\n",
    "    )\n",
    "else:\n",
    "    tokenized_eval = None\n",
    "\n",
    "print(\"Tokenization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758add26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n"
     ]
    }
   ],
   "source": [
    "# Setup LoRA for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Prepare model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11598ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_22180\\3391738512.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    eval_steps=1000,\n",
    "    save_steps=2000,\n",
    "    eval_strategy=\"steps\" if eval_dataset else \"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True if eval_dataset else False,\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=False,\n",
    "    no_cuda= False\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bb282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='298' max='13320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  298/13320 2:42:43 < 119:18:48, 0.03 it/s, Epoch 0.04/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e58e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def generate_medical_findings(conversation_text):\n",
    "    \"\"\"Generate medical findings from conversation\"\"\"\n",
    "    # Extract just the human part for testing\n",
    "    if \"[|Human|]\" in conversation_text and \"[|AI|]\" in conversation_text:\n",
    "        parts = conversation_text.split(\"[|Human|]\")[1]\n",
    "        human_part = parts.split(\"[|AI|]\")[0].strip()\n",
    "    else:\n",
    "        human_part = conversation_text\n",
    "\n",
    "    prompt = f\"<|im_start|>user\\nExtract essential medical findings and generate a clinical summary from this patient conversation: {human_part}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract assistant response\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        assistant_response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "        if \"<|im_end|>\" in assistant_response:\n",
    "            assistant_response = assistant_response.split(\"<|im_end|>\")[0]\n",
    "        return assistant_response.strip()\n",
    "    else:\n",
    "        return response[len(prompt) :].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a94b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample from test dataset\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if test_conversations:\n",
    "    test_sample = test_conversations[0][\"text\"]\n",
    "    findings = generate_medical_findings(test_sample)\n",
    "    print(f\"Generated Findings:\\n{findings}\")\n",
    "else:\n",
    "    print(\"No test dataset available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd0f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
