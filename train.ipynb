{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba20fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Origin Medical\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c030bb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-0.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Configure 4-bit quantization to save memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True, padding_side=\"right\"\n",
    ")\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db472ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transcribed_json_files():\n",
    "    \"\"\"Find all JSON files in transcribed directory\"\"\"\n",
    "    transcribed_dir = r\"medical_dataset\\transcripts\\transcribed\"\n",
    "\n",
    "    print(f\"Searching for JSON files in: {transcribed_dir}\")\n",
    "\n",
    "    if not os.path.exists(transcribed_dir):\n",
    "        print(f\"Directory not found: {transcribed_dir}\")\n",
    "        print(f\"Current directory: {os.getcwd()}\")\n",
    "        print(\n",
    "            f\"Available directories: {[d for d in os.listdir('.') if os.path.isdir(d)]}\"\n",
    "        )\n",
    "        return []\n",
    "\n",
    "    # Find JSON files recursively\n",
    "    json_files = glob.glob(os.path.join(transcribed_dir, \"**/*.json\"), recursive=True)\n",
    "\n",
    "    if not json_files:\n",
    "        json_files = glob.glob(os.path.join(transcribed_dir, \"*.json\"))\n",
    "\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "    for i, json_file in enumerate(json_files):\n",
    "        print(f\"  {i+1}. {os.path.basename(json_file)}\")\n",
    "\n",
    "    return json_files\n",
    "\n",
    "\n",
    "# Find files\n",
    "json_files = find_transcribed_json_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efed125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenMedicalDataProcessor:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_transcribed_conversations(self, json_files):\n",
    "        \"\"\"Load and process JSON conversations for Qwen 2.5\"\"\"\n",
    "        training_examples = []\n",
    "        successful_files = 0\n",
    "\n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                print(f\"Processing: {os.path.basename(json_file)}\")\n",
    "\n",
    "                with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                conversation = self.json_to_conversation(data)\n",
    "\n",
    "                if conversation and len(conversation.strip()) > 50:\n",
    "                    examples = self.create_qwen_training_pairs(conversation, json_file)\n",
    "                    training_examples.extend(examples)\n",
    "                    successful_files += 1\n",
    "                    print(f\"Created {len(examples)} examples\")\n",
    "                else:\n",
    "                    print(f\"Skipped - conversation too short\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        print(f\"\\nSuccessfully processed {successful_files}/{len(json_files)} files\")\n",
    "        print(f\"Total training examples: {len(training_examples)}\")\n",
    "        return training_examples\n",
    "\n",
    "    def json_to_conversation(self, data):\n",
    "        \"\"\"Convert JSON to conversation text\"\"\"\n",
    "        if isinstance(data, list):\n",
    "            conversation_lines = []\n",
    "            for turn in data:\n",
    "                speaker = turn.get(\"speaker\", \"Unknown\")\n",
    "                dialogue_lines = turn.get(\"dialogue\", [])\n",
    "                full_dialogue = \" \".join(str(line) for line in dialogue_lines)\n",
    "\n",
    "                speaker_label = \"Doctor\" if speaker == 1 else \"Patient\"\n",
    "                conversation_lines.append(f\"{speaker_label}: {full_dialogue}\")\n",
    "\n",
    "            return \"\\n\".join(conversation_lines)\n",
    "        else:\n",
    "            return str(data)\n",
    "\n",
    "    def create_qwen_training_pairs(self, conversation, source_file):\n",
    "        \"\"\"Create training examples in Qwen 2.5 chat format\"\"\"\n",
    "        examples = []\n",
    "        conversation_preview = conversation[:1000]  # Limit length\n",
    "\n",
    "        # Qwen 2.5 uses special chat format\n",
    "        # Example 1: Summary generation\n",
    "        summary_messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a medical AI assistant. Generate concise clinical summaries from doctor-patient conversations.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Please summarize this medical conversation:\\n\\n{conversation_preview}\",\n",
    "            },\n",
    "        ]\n",
    "        summary_text = self.format_qwen_chat(summary_messages)\n",
    "        examples.append(\n",
    "            {\n",
    "                \"text\": summary_text,\n",
    "                \"type\": \"summary\",\n",
    "                \"source\": os.path.basename(source_file),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Example 2: Information extraction\n",
    "        extraction_messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a medical AI assistant. Extract structured medical information from conversations.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Extract key medical information from this conversation:\\n\\n{conversation_preview}\\n\\nPlease provide:\\n- Symptoms\\n- Clinical findings\\n- Potential diagnoses\\n- Recommendations\",\n",
    "            },\n",
    "        ]\n",
    "        extraction_text = self.format_qwen_chat(extraction_messages)\n",
    "        examples.append(\n",
    "            {\n",
    "                \"text\": extraction_text,\n",
    "                \"type\": \"extraction\",\n",
    "                \"source\": os.path.basename(source_file),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return examples\n",
    "\n",
    "    def format_qwen_chat(self, messages):\n",
    "        \"\"\"Format messages in Qwen 2.5 chat format\"\"\"\n",
    "        formatted_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                formatted_text += (\n",
    "                    f\"<|im_start|>system\\n{message['content']}<|im_end|>\\n\"\n",
    "                )\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                formatted_text += f\"<|im_start|>user\\n{message['content']}<|im_end|>\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                formatted_text += (\n",
    "                    f\"<|im_start|>assistant\\n{message['content']}<|im_end|>\\n\"\n",
    "                )\n",
    "\n",
    "        # Add assistant start for completion\n",
    "        formatted_text += \"<|im_start|>assistant\\n\"\n",
    "        return formatted_text\n",
    "\n",
    "\n",
    "# Initialize processor\n",
    "print(\"Initializing Qwen data processor...\")\n",
    "data_processor = QwenMedicalDataProcessor(tokenizer)\n",
    "\n",
    "if json_files:\n",
    "    training_examples = data_processor.load_transcribed_conversations(json_files)\n",
    "else:\n",
    "    print(\"‚ùå No JSON files found\")\n",
    "    training_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ffa5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 142 examples\n",
      "\n",
      " Sample training example:\n",
      "<|im_start|>system\n",
      "You are a medical AI assistant. Generate concise clinical summaries from doctor-patient conversations.<|im_end|>\n",
      "<|im_start|>user\n",
      "Please summarize this medical conversation:\n",
      "\n",
      "Doctor: I am the psychiatrist here in this department.\n",
      "Patient: I came to see you because my GP sent me to...\n",
      "üìö Training samples: 127\n",
      "üß™ Validation samples: 15\n",
      "üîÑ Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [00:00<00:00, 1693.37 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 1666.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_for_qwen(examples, tokenizer, max_length=1024):\n",
    "    \"\"\"Tokenize for Qwen 2.5 format\"\"\"\n",
    "    texts = examples[\"text\"]\n",
    "\n",
    "    # Tokenize with Qwen's special tokens\n",
    "    tokenized = tokenizer(\n",
    "        texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # For causal LM, labels are same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "if training_examples:\n",
    "    dataset = Dataset.from_list(training_examples)\n",
    "    print(f\"Dataset created with {len(dataset)} examples\")\n",
    "\n",
    "    # Show sample\n",
    "    print(\"\\n Sample training example:\")\n",
    "    sample_text = (\n",
    "        dataset[0][\"text\"][:300] + \"...\"\n",
    "        if len(dataset[0][\"text\"]) > 300\n",
    "        else dataset[0][\"text\"]\n",
    "    )\n",
    "    print(sample_text)\n",
    "\n",
    "    # Split dataset\n",
    "    if len(dataset) > 1:\n",
    "        train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        train_dataset = train_test_split[\"train\"]\n",
    "        eval_dataset = train_test_split[\"test\"]\n",
    "    else:\n",
    "        train_dataset = dataset\n",
    "        eval_dataset = dataset\n",
    "\n",
    "    print(f\"üìö Training samples: {len(train_dataset)}\")\n",
    "    print(f\"üß™ Validation samples: {len(eval_dataset)}\")\n",
    "\n",
    "    # Tokenize\n",
    "    print(\"üîÑ Tokenizing datasets...\")\n",
    "    tokenized_train = train_dataset.map(\n",
    "        lambda x: tokenize_for_qwen(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "\n",
    "    tokenized_eval = eval_dataset.map(\n",
    "        lambda x: tokenize_for_qwen(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names,\n",
    "    )\n",
    "\n",
    "    print(\"Tokenization completed!\")\n",
    "\n",
    "else:\n",
    "    print(\"No training examples - creating minimal dataset\")\n",
    "    # Create minimal example for testing\n",
    "    minimal_example = {\n",
    "        \"text\": \"<|im_start|>system\\nYou are a medical AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize this medical conversation.<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        \"type\": \"summary\",\n",
    "        \"source\": \"minimal\",\n",
    "    }\n",
    "    dataset = Dataset.from_list([minimal_example])\n",
    "    tokenized_train = dataset.map(\n",
    "        lambda x: tokenize_for_qwen(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    tokenized_eval = tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ac3d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for LoRA training...\n",
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA for Qwen 2.5\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Attention and MLP layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2fdb48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer configured successfully!\n",
      "üìä Will train for 5 epochs\n",
      "üìä Batch size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_16088\\2515700447.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Training arguments optimized for Qwen 2.5\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2.5-medical-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=1e-3,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured successfully!\")\n",
    "print(f\"üìä Will train for {training_args.num_train_epochs} epochs\")\n",
    "print(f\"üìä Batch size: {training_args.per_device_train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ca6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Qwen 2.5 fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Origin Medical\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 02:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Fine-tuning completed successfully!\n",
      "üíæ Model saved to: ./qwen2.5-medical-finetuned\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting Qwen 2.5 fine-tuning...\")\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(\"./qwen2.5-medical-finetuned\")\n",
    "\n",
    "    print(\"Fine-tuning completed successfully!\")\n",
    "    print(\"Model saved to: ./qwen2.5-medical-finetuned\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"Tips:\")\n",
    "    print(\"   - Check if you have enough GPU memory\")\n",
    "    print(\"   - Try reducing batch size if needed\")\n",
    "    print(\"   - Check your training data format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3581fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuned model loaded!\n",
      "‚ùå Error testing model: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_fine_tuned_qwen():\n",
    "    \"\"\"Test the fine-tuned Qwen 2.5 model\"\"\"\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "\n",
    "        # Load the fine-tuned model\n",
    "        print(\"Loading fine-tuned model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"./qwen2.5-medical-finetuned\", trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        \n",
    "        model = PeftModel.from_pretrained(base_model, \"./qwen2.5-medical-finetuned\")\n",
    "        model.eval()\n",
    "\n",
    "        print(\"‚úÖ Fine-tuned model loaded!\")\n",
    "\n",
    "        # Test conversation\n",
    "        test_conversation = \"\"\"Doctor: What brings you in today?\n",
    "Patient: I've been having persistent headaches and fatigue.\n",
    "Doctor: How long has this been going on?\n",
    "Patient: About three weeks now. It's affecting my work.\"\"\"\n",
    "\n",
    "        # Format in Qwen chat format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a medical AI assistant. Generate concise clinical summaries.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Please summarize this medical conversation:\\n\\n{test_conversation}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Format the prompt\n",
    "        prompt = data_processor.format_qwen_chat(messages)\n",
    "\n",
    "        # Generate\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "        # Extract just the assistant's response\n",
    "        if \"<|im_start|>assistant\" in response:\n",
    "            assistant_response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "            if \"<|im_end|>\" in assistant_response:\n",
    "                assistant_response = assistant_response.split(\"<|im_end|>\")[0]\n",
    "            assistant_response = assistant_response.strip()\n",
    "        else:\n",
    "            assistant_response = response[len(prompt) :].strip()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üß™ FINE-TUNED QWEN 2.5 TEST\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üí¨ Input conversation:\\n{test_conversation}\")\n",
    "        print(f\"\\nüìã Generated summary:\\n{assistant_response}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing model: {e}\")\n",
    "\n",
    "\n",
    "# Test if model exists\n",
    "if os.path.exists(\"./qwen2.5-medical-finetuned\"):\n",
    "    test_fine_tuned_qwen()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No fine-tuned model found to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e71bee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Qwen RAG system...\n",
      "‚ùå Error initializing RAG: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QwenMedicalRAG' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     62\u001b[39m     rag_system = QwenMedicalRAG()\n\u001b[32m     64\u001b[39m     test_conv = \u001b[33m\"\"\"\u001b[39m\u001b[33mDoctor: How can I help you today?\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[33mPatient: I\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve been feeling very anxious and having trouble sleeping.\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[33mDoctor: When did this start?\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[33mPatient: About a month ago, after I changed jobs.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     summary = \u001b[43mrag_system\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_medical_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_conv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí¨ Test conversation:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtest_conv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìã RAG Summary:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msummary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mQwenMedicalRAG.generate_medical_summary\u001b[39m\u001b[34m(self, conversation, max_tokens)\u001b[39m\n\u001b[32m     23\u001b[39m messages = [\n\u001b[32m     24\u001b[39m     {\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     },\n\u001b[32m     32\u001b[39m ]\n\u001b[32m     34\u001b[39m prompt = data_processor.format_qwen_chat(messages)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m inputs = tokenizer.encode(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m.device)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     38\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.model.generate(\n\u001b[32m     39\u001b[39m         inputs,\n\u001b[32m     40\u001b[39m         max_new_tokens=max_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m         pad_token_id=tokenizer.eos_token_id,\n\u001b[32m     44\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'QwenMedicalRAG' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "class QwenMedicalRAG:\n",
    "    def __init__(self, model_path=\"./qwen2.5-medical-finetuned\"):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_path, trust_remote_code=True\n",
    "            )\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "            )\n",
    "            from peft import PeftModel\n",
    "\n",
    "            self.model = PeftModel.from_pretrained(base_model, model_path)\n",
    "            self.model.eval()\n",
    "            print(\"‚úÖ Qwen RAG system initialized!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error initializing RAG: {e}\")\n",
    "\n",
    "    def generate_medical_summary(self, conversation, max_tokens=300):\n",
    "        \"\"\"Generate medical summary using fine-tuned Qwen\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a medical AI assistant. Generate accurate clinical summaries from doctor-patient conversations.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Summarize this medical conversation:\\n\\n{conversation}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        prompt = data_processor.format_qwen_chat(messages)\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "        # Extract assistant response\n",
    "        if \"<|im_start|>assistant\" in response:\n",
    "            return (\n",
    "                response.split(\"<|im_start|>assistant\")[-1]\n",
    "                .split(\"<|im_end|>\")[0]\n",
    "                .strip()\n",
    "            )\n",
    "        else:\n",
    "            return response[len(prompt) :].strip()\n",
    "\n",
    "\n",
    "# Test RAG system\n",
    "if os.path.exists(\"./qwen2.5-medical-finetuned\"):\n",
    "    print(\"üîÑ Testing Qwen RAG system...\")\n",
    "    rag_system = QwenMedicalRAG()\n",
    "\n",
    "    test_conv = \"\"\"Doctor: How can I help you today?\n",
    "Patient: I've been feeling very anxious and having trouble sleeping.\n",
    "Doctor: When did this start?\n",
    "Patient: About a month ago, after I changed jobs.\"\"\"\n",
    "\n",
    "    summary = rag_system.generate_medical_summary(test_conv)\n",
    "    print(f\"\\nüí¨ Test conversation:\\n{test_conv}\")\n",
    "    print(f\"\\nüìã RAG Summary:\\n{summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
